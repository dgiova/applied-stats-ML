{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "param_estimation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOXqOceJTmpc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3caaaf2b-f978-4bfd-d70b-c8ee8bcbaa8a"
      },
      "source": [
        "import random\n",
        "import math\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import scipy\n",
        "from scipy.optimize import minimize\n",
        "from scipy.stats import gamma, norm\n",
        "import seaborn as sns\n",
        "import time\n",
        "\n",
        "from functools import partial"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVnGMiTiUDe3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N = int(1e5)  # MC iterations\n",
        "def _r(x): return np.round(x, 2)\n",
        "def _CI(mu, std, n=1, z=1): return mu + (z/np.sqrt(n)) * np.array([-std, std])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VropIMJLkNoF",
        "colab_type": "text"
      },
      "source": [
        "# Parameter Estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2a9jh9yGLHb",
        "colab_type": "text"
      },
      "source": [
        "## MLE for Gamma distribution with initialization values from MoMe\n",
        "\n",
        "Let's find the MLE of Gamma parameters $\\alpha$ (shape) and $\\lambda$ (rate) starting from MoM estimators. We'll simulate 100 data points from a Gamma with $\\alpha_0 = 3$ and $\\lambda_0 = 0.5$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TobKjCgGKHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha = 3\n",
        "lambd = 0.5\n",
        "n = 100\n",
        "X = gamma.rvs(alpha, scale=1/lambd, size=n)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bum5SAHfH95q",
        "colab_type": "text"
      },
      "source": [
        "The MoMe of a Gamma distribution are:\n",
        "$$\\hat{\\lambda} = \\frac{\\bar{X}_n}{s^2_n}$$\n",
        "$$\\hat{\\alpha} = \\frac{\\bar{X}_n^2}{s_n^2}$$\n",
        "these will be our starting guesses for the MLE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HwGjoFrHuZd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c40fbbbe-2ea4-44df-f5b5-eb37325ecb2f"
      },
      "source": [
        "alpha_mom = np.mean(X)**2 / np.var(X)\n",
        "lambd_mom = np.mean(X) / np.var(X)\n",
        "print('parameters estimated from MoM:')\n",
        "print('alpha:', alpha_mom)\n",
        "print('lambda:', lambd_mom)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parameters estimated from MoM:\n",
            "alpha: 2.9192312119021797\n",
            "lambda: 0.47378421404346205\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdLlgzAjJHPy",
        "colab_type": "text"
      },
      "source": [
        "We can see that the MoMe guesses are pretty close to the true paramters. \\\\\n",
        "Let's write down the negative log likelhood and minimize it for w.r.t. $\\alpha$ and $\\lambda$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64oBa4HIIsE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gamma_neg_log_lik(params):\n",
        "  alpha, lambd = params\n",
        "  return -(n*alpha*np.log(lambd) + (alpha-1)*sum(np.log(X)) \\\n",
        "           - lambd*sum(X) - n*np.log(scipy.special.gamma(alpha)))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2vDWBF3Kau4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f4f936d7-94f9-4504-807a-e1b2075d6e25"
      },
      "source": [
        "params = (alpha_mom, lambd_mom)\n",
        "bnds = ((0, None), (0, None))\n",
        "result = minimize(gamma_neg_log_lik, params, bounds=bnds, method='L-BFGS-B')\n",
        "print(result.x)\n",
        "print('Number of iterations:', result.nit)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.75332426 0.4468579 ]\n",
            "Number of iterations: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSI-7yiHRk2G",
        "colab_type": "text"
      },
      "source": [
        "We can see that this result is better than MoMe (very few times it's worse, we'll build CI to understand the variability better). Also, the number of iterations performed is very limited as we were expecting: starting from good guesses of the parameters, the algorithm converges quickly. \\\\\n",
        "Let's now build confidence intervals for the estimated parameters through **parametric bootstrap** and compare MoMe and MLE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3FAxdE1PXIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N = int(1e3)\n",
        "alpha = 3\n",
        "lambd = 0.5\n",
        "n = 100\n",
        "bnds = ((0, None), (0, None))\n",
        "MoMe = []  # will contain deviations\n",
        "MLE = []\n",
        "for _ in range(N):\n",
        "  X = gamma.rvs(alpha, scale=1/lambd, size=n)\n",
        "  # MoMe\n",
        "  alpha_mom = np.mean(X)**2 / np.var(X)\n",
        "  lambd_mom = np.mean(X) / np.var(X)\n",
        "  # directly computing the deviation from true params here\n",
        "  MoMe.append((alpha_mom-alpha, lambd_mom-lambd))\n",
        "  # MLE\n",
        "  result = minimize(gamma_neg_log_lik, (alpha_mom, lambd_mom), \n",
        "                    bounds=bnds, method='L-BFGS-B')\n",
        "  alpha_mle, lambd_mle = result.x\n",
        "  MLE.append((alpha_mle-alpha, lambd_mle-lambd))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZUGHokQTc3W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5519e662-ba91-4df3-8d30-d77be19a0a42"
      },
      "source": [
        "CI = lambda x, dev: x + np.array([np.percentile(dev, 5),\n",
        "                                  np.percentile(dev, 95)])\n",
        "# MoMe and MLE deviations from true paramter\n",
        "alpha_mom_dev, lambd_mom_dev = zip(*MoMe)\n",
        "alpha_mle_dev, lambd_mle_dev = zip(*MLE)\n",
        "print('90% CI:')\n",
        "print('alpha (MoMe):', CI(alpha, alpha_mom_dev))\n",
        "print('lambda (MoMe):', CI(lambd, lambd_mom_dev))\n",
        "print('alpha (MLE):', CI(alpha, alpha_mle_dev))\n",
        "print('lambda (MLE):', CI(lambd, lambd_mle_dev))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "90% CI:\n",
            "alpha (MoMe): [2.37836845 4.00967412]\n",
            "lambda (MoMe): [0.39135354 0.68556171]\n",
            "alpha (MLE): [2.46183098 3.83111037]\n",
            "lambda (MLE): [0.39930414 0.64965277]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iC27a7M3XGDo",
        "colab_type": "text"
      },
      "source": [
        "We can clearly see that the MLE estimates have lower variance than the MoMe as the CI are much narrower! \\\\\n",
        "This is in line with the fact that MLE maximizes **statistical efficiency**, i.e. the MSE of the estimator $E_{\\theta_0}(\\hat{\\theta}_n - \\theta_0)^2$ is minimized when $\\hat{\\theta}_n$ is chosen to be the MLE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC9ZZQ9fi7bt",
        "colab_type": "text"
      },
      "source": [
        "## Plug-in estimators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIRsd34djI_W",
        "colab_type": "text"
      },
      "source": [
        "Suppose that we wish to estimate the $p^{th}$ quantile $q_0$ of an exponentially distributed population, in the presence of a random sample $X_1, X_2, \\dots , X_n$. \\\\\n",
        "For an exponential population with parameter $\\lambda$, we have $q = -\\frac{1}{\\lambda} log(1 âˆ’ p)$. \\\\\n",
        "We know that the MLE for $\\lambda_0$ is: $\\hat{\\lambda} = \\frac{1}{\\overline{X}_n}$. \\\\\n",
        "Thus, according to the plug-in principle, the most statistically efficient way to estimate the quantile is to plug-in the MLE in the quantile equation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbjvKAfJjBSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lambd = 2\n",
        "n = 1000\n",
        "X = np.random.exponential(scale=1/lambd, size=n)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXAp0lKPk4BM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0338c18c-f508-4ca7-d0ac-66a5d2ee1b04"
      },
      "source": [
        "lambd_mle = 1 / np.mean(X)\n",
        "print('Lambda from MLE:', lambd_mle)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lambda from MLE: 2.077998166798869\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efBEdZV2k9dR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "01aac78c-f725-43ae-a6c2-a46203795d38"
      },
      "source": [
        "p = 0.25\n",
        "q_hat = lambda lambd, p: -(1/lambd) * np.log(1-p)\n",
        "print('Estimated quantile through MLE plug-in:', q_hat(lambd_mle, p))\n",
        "print('Observed quantile from data:', np.percentile(X, p*100))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Estimated quantile through MLE plug-in: 0.13844192793247342\n",
            "Observed quantile from data: 0.13020970230653706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IGZV1Qjpzt-",
        "colab_type": "text"
      },
      "source": [
        "We now want to construct large-sample CI for the true quantile $q_0$. We know that:\n",
        "$$\\sqrt{n}(\\hat{q}_n - q_0) \\Rightarrow \\sigma log(1-p)^{-1}N(0,1)$$\n",
        "where $\\sigma^2 = Var_{\\theta_0}(X_1)$; thus we need the the variance of the X's. Firstly, we might think about estimating it through the sample variance, but we also know that for the exponential distribution\n",
        "$$Var(X) = \\frac{1}{\\lambda^2}$$\n",
        "so by plug-in we have that:\n",
        "$$\\hat{Var(X)} = \\overline{X}^2$$\n",
        "Let's compare the two"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XwpxCLblaFz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d3ee013e-fc29-49ad-a86f-15dd027413dd"
      },
      "source": [
        "print('Sample variance:', np.var(X))\n",
        "print('Plug-in variance:', np.mean(X)**2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample variance: 0.23424834092638047\n",
            "Plug-in variance: 0.23158460196519076\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uSZC6xWqpjn",
        "colab_type": "text"
      },
      "source": [
        "So our 95% CI is given by:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5jDeNqGqmjh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1203be56-e983-402f-c91e-7d880db3d2db"
      },
      "source": [
        "z = 1.96\n",
        "CI = np.mean(X)*np.log((1-p)**-1) * np.array([1-z/np.sqrt(n), 1+z/np.sqrt(n)])\n",
        "print('95% Confidence Interval for lambda:', CI)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "95% Confidence Interval for lambda: [0.12986121 0.14702265]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xqt6HnhiwSor",
        "colab_type": "text"
      },
      "source": [
        "Let's check that this is really 95% through MC simulation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XawgiVW2vJNB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "78672975-b3f4-4b2b-8003-e0a54dd0301b"
      },
      "source": [
        "lambd = 2\n",
        "n = 1000\n",
        "p = 0.25\n",
        "q_hat = lambda lambd, p: -(1/lambd) * np.log(1-p)\n",
        "N = int(1e3)\n",
        "Z = np.zeros(N)\n",
        "for i in range(N):\n",
        "  X = np.random.exponential(scale=1/lambd, size=n)\n",
        "  lambd_mle = 1 / np.mean(X)\n",
        "  q_MC = q_hat(lambd_mle, p)\n",
        "  if CI[0] <= q_MC <= CI[1]:\n",
        "    Z[i] = 1\n",
        "print('Observed probability that q_MC is contained in the CI:', np.mean(Z))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observed probability that q_MC is contained in the CI: 0.75\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}